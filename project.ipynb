{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cf28c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from models.tests import build_scale\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd77f4",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f874ff3",
   "metadata": {},
   "source": [
    "The dataset used for this project is 6 years old and was downloaded from Kaggle user Shivam Raj:\\\n",
    "https://www.kaggle.com/datasets/raj5287/abc-notation-of-tunes\n",
    "\n",
    "The data is a `.txt` file that contains songs that have been transcribed into ABC notation, where each entry is separated by 2 newlines. The first set of lines contains the metadata fields:\\\n",
    "`X`: ID\\\n",
    "`T`: Title\\\n",
    "`M`: Meter\\\n",
    "`L`: Unit Note Length\\\n",
    "`B`: Background Information\\\n",
    "`N`: Notes\\\n",
    "`Z`: Transcription\\\n",
    "`K`: Key\n",
    "\n",
    "These headers are then followed by the transcribed melody. Non-note symbols are used to indicate modifications to note durations, accidentals, and ornamentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a3e081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 1\n",
      "T: The Enchanted Valley\n",
      "M: 2/4\n",
      "L: 1/16\n",
      "B: \"O'Neill's 1\"\n",
      "N: \"Very slow\" \"collected by J. O'Neill\"\n",
      "N:\n",
      "Z: \"Transcribed by Norbert Paap, norbertp@bdu.uva.nl\"\n",
      "Z:\n",
      "K:Gm\n",
      "G3-A (Bcd=e) | f4 (g2dB) | ({d}c3-B) G2-E2 | F4 (D2=E^F) |\n",
      "G3-A (Bcd=e) | f4 d2-f2 | (g2a2 b2).g2 | {b}(a2g2 f2).d2 |\n",
      "(d2{ed}c2) B2B2 | (A2G2 {AG}F2).D2 | (GABc) (d2{ed}c>A) | G2G2 G2z ||\n",
      "G | B2c2 (dcAB) | G2G2 G3G | B2d2 (gfdc) | d2g2 (g3ga) |\n",
      "(bagf) (gd)d>c | (B2AG) F-D.D2 | (GABc) d2d2 | (bgfd) cA.F2 |\n",
      "G2A2 (B2{cB}AG) | A3-G F2-D2 | (GABc) (d2{ed}c>A) | G2G2 G2z2 ||\n",
      "\n",
      "\n",
      "X: 2\n",
      "T: Fare You Well\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "output_dir = os.path.join(os.getcwd(), 'output')\n",
    "\n",
    "with open(os.path.join(data_dir, 'raw', 'abc_notations.txt' ), 'r') as f:\n",
    "    raw_lines = f.readlines()\n",
    "    for line in raw_lines[:20]:\n",
    "        print(line.strip())\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c5371c",
   "metadata": {},
   "source": [
    "The raw `abc_notations.txt` is standardized then parsed into a json file named `songs_dict.json`. Certain fields that were not considered to be musically significant, such as background information, were dropped from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c14781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\n",
      "title: The Enchanted Valley\n",
      "time_signature: 2/4\n",
      "note_length: 1/16\n",
      "book: \"O'Neill's 1\"\n",
      "notes: \"Very slow\" \"collected by J. O'Neill\"\n",
      "transcription: \"Transcribed by Norbert Paap, norbertp@bdu.uva.nl\"\n",
      "key: Gm\n",
      "G3-A Bcde|f4 g2dB|dc3-B G2-E2|F4 D2EF|\n",
      "G3-A Bcde|f4 d2-f2|g2a2 b2.g2|ba2g2 f2.d2|\n",
      "d2edc2 B2B2|A2G2 AGF2.D2|GABc d2edcA|G2G2 G2z||\n",
      "G|B2c2 dcAB|G2G2 G3G|B2d2 gfdc|d2g2 g3ga|\n",
      "bagf gddc|B2AG F-D.D2|GABc d2d2|bgfd cA.F2|\n",
      "G2A2 B2cBAG|A3-G F2-D2|GABc d2edcA|G2G2 G2z2||\n"
     ]
    }
   ],
   "source": [
    "with(open(os.path.join(data_dir, 'lookup_tables', 'songs_dict.json'), 'r')) as f:\n",
    "    songs_dict = json.load(f)\n",
    "    entry = songs_dict['1']\n",
    "    for key, value in entry.items():\n",
    "        if key =='melody':\n",
    "            for line in value:\n",
    "                print(line)\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7130c",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb9b23a",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ca9e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3108\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vocabulary adapted from:\n",
    "    https://abcnotation.com/wiki/abc:standard:v2.2\n",
    "\"\"\"\n",
    "NOTES = \"ABCDEFGabcdefg\"\n",
    "ACCIDENTALS = \"^_=\"\n",
    "DECORATIONS = \"~HLMOPSTuv\"\n",
    "DOT = \".\"\n",
    "RESTS = \"z\"\n",
    "BARS = ['|', '||', '|:', ':|', ':||']\n",
    "TIE = '-'\n",
    "SLUR = '()'\n",
    "STRUCTURAL = ['K:', 'L:', 'M:']\n",
    "DURATIONS = \"23468\"\n",
    "SPECIAL = {\n",
    "    'PAD': '[PAD]',\n",
    "    'UNK': '[UNK]',\n",
    "    'START': '[START]',\n",
    "    'END': '[END]'\n",
    "}\n",
    "\n",
    "PREFIXED = [f\"{p}{note}\" for note in (NOTES + RESTS) for p in ACCIDENTALS + DOT + DECORATIONS]\n",
    "SUFFIXED = [f\"{note}{duration}\" for note in (NOTES + RESTS) for duration in DURATIONS + DOT]\n",
    "\n",
    "TRIPLET_PREFIX = [f\"3{a}{b}{c}\" for a in NOTES for b in NOTES for c in NOTES]\n",
    "\n",
    "OCTAVE_NOTES = [f\"{note},\" for note in (NOTES + RESTS)]\n",
    "OCTAVE_NOTES += [f\"{note}'\" for note in (NOTES + RESTS)]\n",
    "\n",
    "FRACTIONAL_NOTES = [f\"{note}/{dur}\" for note in (NOTES + ''.join(SUFFIXED)) for dur in DURATIONS]\n",
    "FRACTIONAL_RESTS = [f\"z/{dur}\" for dur in DURATIONS]\n",
    "\n",
    "VOCAB = list(SPECIAL.values()) + list(NOTES) + PREFIXED + SUFFIXED + STRUCTURAL + BARS +\\\n",
    "        [TIE, '(', ')'] + TRIPLET_PREFIX + OCTAVE_NOTES + FRACTIONAL_RESTS\n",
    "\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b8aab7",
   "metadata": {},
   "source": [
    "A custom tokenizer is used to break up input sequences into meaningful tokens, which is often more than just individual notes. For example, triplets, accidentals, and note durations are treated as single tokens if they match known patterns in the vocabulary.\n",
    "\n",
    "Each entry from the JSON file is processed field by field. Important metadata such as title, time signature, note length, and key are inserted at the start of the sequence. The melody is then tokenized line-by-line, and the full token sequence is converted into a tensor of indices using the predefined vocabulary.\n",
    "\n",
    "This representation allows the model to learn from both musical structure and contextual metadata.\n",
    "For example, the average Seq2Seq accuracy of the model outputs increased from around 20% to 52% by introducing composer and rythym metadata into the model, along with other improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8129d415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Tokens:\n",
      "tensor([  1,   1,   1,   1,   1, 265, 326,   4,   1,   5,  13,  14,  15, 321,\n",
      "        302,   1, 306,  14,   5, 321,  14, 283, 326,   5,   1, 264, 326, 252,\n",
      "        321, 260,   1, 246,   8,   9, 321, 265, 326,   4,   1,   5,  13,  14,\n",
      "         15, 321, 302,   1, 288, 326, 300, 321, 306, 270,   1, 276, 203,   1,\n",
      "        321,  12, 270, 306,   1, 300, 161,   1, 321, 288,  15,  14, 282,   1,\n",
      "        234, 234, 321, 228, 264,   1,   4,  10, 258,  63,   1, 321,  10,   4,\n",
      "          5,  13,   1, 288,  15,  14,  13,   4, 321, 264, 264,   1, 264,   1,\n",
      "        322,  10, 321, 234, 282,   1,  14,  13,   4,   5, 321, 264, 264,   1,\n",
      "        265,  10, 321, 234, 288,   1,  17,  16,  14,  13, 321, 288, 306,   1,\n",
      "        307,  17,  11, 321,  12,  11,  17,  16,   1,  17,  14,  14,  13, 321,\n",
      "        234,   4,  10,   1,   9, 326, 251, 246, 321,  10,   4,   5,  13,   1,\n",
      "        288, 288, 321,  12,  17,  16,  14,   1,  13, 233, 258, 321, 264, 228,\n",
      "          1, 234,  13,   5,   4,  10, 321, 229, 326,  10,   1, 258, 326, 246,\n",
      "        321,  10,   4,   5,  13,   1, 288,  15,  14,  13,   4, 321, 264, 264,\n",
      "          1, 264, 312, 322,   3])\n"
     ]
    }
   ],
   "source": [
    "from models.dataset import tok2ind\n",
    "\n",
    "PAD = SPECIAL['PAD']\n",
    "UNK = SPECIAL['UNK']\n",
    "START = SPECIAL['START']\n",
    "END = SPECIAL['END']\n",
    "EXCLUDED_ENTRIES = {'1850'}\n",
    "\n",
    "def tokenize_melody(melody):\n",
    "    all_tokens = []\n",
    "\n",
    "    for line in melody:\n",
    "        line = line.strip().replace('x', 'z')   # normalize 'x' to 'z'\n",
    "        if line.startswith('C:'):   # save composer information where available\n",
    "            composer = line[2:].strip()\n",
    "            all_tokens.append(f\"C:{composer} \") if composer not in all_tokens else None\n",
    "            continue\n",
    "        if line.startswith('R:'):   # save rhythm information where available\n",
    "            rhythm = line[2:].strip()\n",
    "            all_tokens.append(f\"R:{rhythm} \") if rhythm not in all_tokens else None\n",
    "            continue\n",
    "\n",
    "        line_tokens = tokenize_line(line)\n",
    "        all_tokens.extend(line_tokens)\n",
    "\n",
    "    return all_tokens\n",
    "\n",
    "\n",
    "def tokenize_line(line):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "\n",
    "    vocab_sort = sorted(VOCAB, key=len, reverse=True)\n",
    "    while i < len(line):\n",
    "        match = None\n",
    "\n",
    "        if i + 3 <= len(line) and line[i:i+3] in TRIPLET_PREFIX:\n",
    "            match = line[i:i+3]\n",
    "            tokens.append(match.strip())\n",
    "            i += 3\n",
    "            continue\n",
    "\n",
    "        for token in vocab_sort:\n",
    "            if line.startswith(token, i):\n",
    "                match = token\n",
    "                break\n",
    "\n",
    "        if match:\n",
    "            tokens.append(match.strip())\n",
    "            i += len(match)\n",
    "        else:\n",
    "            if i + 2 < len(line) and line[i + 1] == '/' and line[i + 2].isdigit():\n",
    "                # fractional duration\n",
    "                note = line[i]\n",
    "                fraction = line[i:i + 3]\n",
    "                tokens.append(note)\n",
    "                tokens.append(fraction)\n",
    "                i += 3\n",
    "            else:\n",
    "                tokens.append(UNK)\n",
    "                i += 1\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def entry2tensor(entry):\n",
    "    tokens = []\n",
    "    metadata = {}\n",
    "\n",
    "    if 'title' in entry:\n",
    "        val = entry['title']\n",
    "        tokens.append(f\"N:{val} \")\n",
    "        metadata['N:'] = val\n",
    "\n",
    "    if 'time_signature' in entry:\n",
    "        val = entry['time_signature']\n",
    "        tokens.append(f\"T:{val} \")\n",
    "        metadata['T:'] = val\n",
    "\n",
    "    if 'note_length' in entry:\n",
    "        val = entry['note_length']\n",
    "        tokens.append(f\"L:{val} \")\n",
    "        metadata['L:'] = val\n",
    "\n",
    "    if 'key' in entry:\n",
    "        val = entry['key']\n",
    "        tokens.append(f\"K:{val} \")\n",
    "        metadata['K:'] = val\n",
    "        \n",
    "    tokens.append(f'{START} ')\n",
    "\n",
    "    melody = entry.get('melody', '')\n",
    "    melody_tokens = tokenize_melody(melody)\n",
    "    tokens.extend(melody_tokens)\n",
    "\n",
    "    tokens.append(END)\n",
    "\n",
    "    # print(f\"Tokens:\\n{(''.join(tokens))[:125] + ' ...'}\")\n",
    "\n",
    "    indices = [tok2ind(tok) for tok in tokens]\n",
    "\n",
    "    return torch.tensor(indices, dtype=torch.long), metadata\n",
    "\n",
    "tensor, metadata = entry2tensor(entry)\n",
    "print(f\"Converted Tokens:\\n{tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b5aa75",
   "metadata": {},
   "source": [
    "```\n",
    "Tokens:\n",
    "N:The Enchanted Valley T:2/4 L:1/16 K:Gm [START] G3-A[UNK]Bcde|f4[UNK]g2dB|dc3-B[UNK]G2-E2|F4[UNK]D2EF|G3-A[UNK]Bcde|f4[UNK]d ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3454c975",
   "metadata": {},
   "source": [
    "A PyTorch Dataset module is then used to load the entries from the JSON file and:\\\n",
    "     - tokenize it\\\n",
    "     - optionally augment it by shifting the key\\\n",
    "     - split it into training, validation, and test subsets\\\n",
    "     - generate input/target tensors for to feed into the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680af384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABCDataset(Dataset):\n",
    "    def __init__(self, json_file, augment_data=False, transpose_range=(-2, 3)):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        self.file_name = json_file\n",
    "        self.sequences = []\n",
    "        self.entry_indices = list(self.data.keys())\n",
    "        self.metadata = []\n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        self.vocab = VOCAB\n",
    "        self.pad_token = PAD\n",
    "        self.pad_idx = VOCAB.index(PAD)\n",
    "        self.entries = []\n",
    "\n",
    "        self.idx2char_dict = {i: ch for i, ch in enumerate(VOCAB)}\n",
    "\n",
    "        self.augment_data = augment_data\n",
    "        self.transpose_range = transpose_range\n",
    "\n",
    "        for idx in self.entry_indices:\n",
    "            if str(idx) in EXCLUDED_ENTRIES:\n",
    "                continue\n",
    "            entry = self.data[idx]\n",
    "            self.entries.append(entry)\n",
    "            entry_t, metadata = entry2tensor(entry)\n",
    "            self.sequences.append(entry_t)\n",
    "            self.metadata.append(metadata)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence_tensor = self.sequences[idx]\n",
    "        entry = self.entries[idx]\n",
    "        key = entry.get('K:', 'C')\n",
    "        if self.augment_data:\n",
    "            shift = np.random.randint(*self.transpose_range)\n",
    "            sequence_tensor = self.transpose_tensor(sequence_tensor, shift, key)\n",
    "\n",
    "        input_tensor = sequence_tensor[:-1]\n",
    "        target_tensor = sequence_tensor[1:]\n",
    "        metadata = self.metadata[idx]\n",
    "\n",
    "        return input_tensor, target_tensor, metadata\n",
    "\n",
    "    def transpose_tensor(self, tensor, steps, key):\n",
    "        transposed = tensor.clone()\n",
    "        for i in range(tensor.size(0)):\n",
    "            idx = torch.argmax(tensor[i]).item()\n",
    "            char = self.vocab[idx]\n",
    "            if char in NOTES:\n",
    "                shifted = self.transpose_note(char, steps, key)\n",
    "                transposed[i] = 0\n",
    "                transposed[i][tok2ind(shifted)] = 1\n",
    "        return transposed\n",
    "\n",
    "    def transpose_note(self, note, shift, key):\n",
    "        scale = build_scale(key)\n",
    "        is_upper = note.isupper()\n",
    "        base = note.upper()\n",
    "\n",
    "        if base not in scale:\n",
    "            return note\n",
    "\n",
    "        i = scale.index(base)\n",
    "        new_i = (i + shift) % len(scale)\n",
    "        transposed = scale[new_i]\n",
    "        return transposed if is_upper else transposed.lower()\n",
    "\n",
    "    def get_pad_idx(self):\n",
    "        return self.pad_idx\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def split_dataset(self, train_ratio=0.8, val_ratio=0.1):\n",
    "        train_size = int(train_ratio * len(self))\n",
    "        val_size = int(val_ratio * len(self))\n",
    "        test_size = len(self) - train_size - val_size\n",
    "        return random_split(self, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8a309",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca9bed",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aced007",
   "metadata": {},
   "source": [
    "Hidden and embedding dimensions were chosen to be equal, with a value of 144 resulting in the best balance between output complexity and structural correctness. Additionally, a temperature of 1.1 was set to increase the creativity of model outputs, as outlined in: https://medium.com/@weidagang/demystifying-temperature-in-machine-learning-ef6828ad4e2d\n",
    "\n",
    "The number of layers for RNN and GRU models worked best with 4, while LSTM overfit quickly with any value greater than 2. \n",
    "A dropout of 0.5 was the sweet spot for our model's ability to decrease training and validation loss consistently and evenly.\n",
    "\n",
    "Batch size was set to 32 for efficient training and a suitable output length is 256. An upper bound of 200 epochs allowed the model to learn with a learning rate of 0.0006 and save the best model through early stopping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e407b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HP:\n",
    "    hidden_dim = 144 \n",
    "    embed_dim = 144\n",
    "    n_layers = 4    # set to 2 for LSTM\n",
    "    # n_layers = 2\n",
    "    dropout = 0.5\n",
    "\n",
    "    batch_size = 32\n",
    "    num_epochs = 200    # with early stopping, i.e. upper bound\n",
    "    lr = 0.0006 \n",
    "    output_len = 256\n",
    "    \n",
    "    # > 1 for more creative < 1 for more rule-following\n",
    "    temp = 1.1\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3833599",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11d4b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx, dropout):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        # print(\"Input shape:\", x.shape)\n",
    "        # print(\"Input dtype:\", x.dtype)\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        logits = self.fc(output)  # [batch_size, seq_len, vocab_size]\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85412a40",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75a4b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        # print(\"Input shape:\", x.shape)\n",
    "        # print(\"Input dtype:\", x.dtype)\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        logits = self.fc(output)  # [batch_size, seq_len, vocab_size]\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f44c2",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "376fd97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx, dropout):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        # print(\"Input shape:\", x.shape)\n",
    "        # print(\"Input dtype:\", x.dtype)\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        logits = self.fc(output)  # [batch_size, seq_len, vocab_size]\n",
    "        return logits, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821db93f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab37d4",
   "metadata": {},
   "source": [
    "The training and validation data is used to train and save models in `train_model()`, then the test data is used to ensure the model did not overfit in `eval_model()`. The trained models are then saved to the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abe33ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, val_loader, num_epochs=3, batch_size=32, learning_rate=0.0005, mtype='rnn'):\n",
    "    device = HP.device\n",
    "    dataset = dataloader.dataset\n",
    "\n",
    "    if isinstance(dataset, Subset):\n",
    "        dataset = dataset.dataset\n",
    "\n",
    "    input_size = dataset.get_vocab_size()\n",
    "    hidden_size = HP.hidden_dim\n",
    "    output_size = input_size\n",
    "\n",
    "    model = None\n",
    "    if mtype == 'rnn':\n",
    "        model = RNNModel(input_size, HP.embed_dim, hidden_size, HP.n_layers, dataset.get_pad_idx(), HP.dropout).to(device)\n",
    "    elif mtype == 'lstm':\n",
    "        model = LSTMModel(input_size, HP.embed_dim, hidden_size, HP.n_layers, dataset.get_pad_idx(), HP.dropout).to(device)\n",
    "    elif mtype == 'gru':\n",
    "        model = GRUModel(input_size, HP.embed_dim, hidden_size, HP.n_layers, dataset.get_pad_idx(), HP.dropout).to(device)\n",
    "\n",
    "    PAD_IDX = dataset.get_pad_idx()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    epochs_no_improvement = 0\n",
    "    records = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for input_tensor, target_tensor, _ in dataloader:\n",
    "            input_tensor, target_tensor = input_tensor.to(device), target_tensor.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output, _ = model(input_tensor)\n",
    "\n",
    "            output = output.reshape(-1, output_size)\n",
    "            target_flat = target_tensor.reshape(-1)\n",
    "\n",
    "            loss = criterion(output, target_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for input_tensor, target_tensor, _ in val_loader:\n",
    "                input_tensor, target_tensor = input_tensor.to(device), target_tensor.to(device)\n",
    "\n",
    "                output, _ = model(input_tensor)\n",
    "\n",
    "                output = output.reshape(-1, output_size)\n",
    "                target_flat = target_tensor.reshape(-1)\n",
    "\n",
    "                loss = criterion(output, target_flat)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "        records.append({'epoch': epoch + 1, 'train_loss': avg_train_loss, 'val_loss': avg_val_loss})\n",
    "\n",
    "        # early stopping\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            epochs_no_improvement = 0\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print(f\"Model saved @ epoch {epoch + 1}\")\n",
    "        else:\n",
    "            epochs_no_improvement += 1\n",
    "            if epochs_no_improvement >= 5:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    if best_model is not None:\n",
    "        torch.save(model.state_dict(), f\"output/{mtype}_model.pth\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss', color='red')\n",
    "    plt.plot(val_losses, label='Validation Loss', color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.title(f\"{mtype.upper()} Training and Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"output/figs/{mtype}_loss.png\")\n",
    "    plt.close()\n",
    "\n",
    "    records_df = pd.DataFrame(records)\n",
    "    records_df.to_csv(f\"output/{mtype}_loss.csv\", index=False)\n",
    "    print(\"Training complete. Model saved.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def eval_model(test_loader, output_dir, model_type='rnn'):\n",
    "    dataset = test_loader.dataset\n",
    "    if isinstance(dataset, Subset):\n",
    "        dataset = dataset.dataset\n",
    "\n",
    "    input_size = dataset.get_vocab_size()\n",
    "    hidden_size = HP.hidden_dim\n",
    "    output_size = input_size\n",
    "    device = HP.device\n",
    "\n",
    "    model = None\n",
    "    if model_type == 'rnn':\n",
    "        model = RNNModel(input_size, HP.embed_dim, hidden_size, HP.n_layers, dataset.get_pad_idx(), HP.dropout).to(device)\n",
    "    if model_type == 'lstm':\n",
    "        model = LSTMModel(input_size, HP.embed_dim, hidden_size, HP.n_layers, dataset.get_pad_idx(), HP.dropout).to(device)\n",
    "    if model_type == 'gru':\n",
    "        model = GRUModel(input_size, HP.embed_dim, hidden_size, HP.n_layers, dataset.get_pad_idx(), HP.dropout).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"output/{model_type}_model.pth\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    PAD_IDX = dataset.get_pad_idx()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    with torch.no_grad():\n",
    "        for input_tensor, target_tensor, metadata in test_loader:\n",
    "            input_tensor, target_tensor = input_tensor.to(device), target_tensor.to(device)\n",
    "            output, _ = model(input_tensor)\n",
    "\n",
    "            output_flat = output.reshape(-1, output_size)\n",
    "            target_flat = target_tensor.reshape(-1)\n",
    "\n",
    "            loss = criterion(output_flat, target_flat)\n",
    "            test_loss += loss.item()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            preds = output_flat.argmax(dim=1)\n",
    "            mask = (target_flat != PAD_IDX)\n",
    "            correct_batch = (preds[mask] == target_flat[mask]).sum().item()\n",
    "            total_batch = mask.sum().item()\n",
    "            acc = 100.0 * correct_batch / total_batch\n",
    "            accuracies.append(acc)\n",
    "            total += total_batch\n",
    "            correct += correct_batch\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100.0 * correct / total\n",
    "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.2f}%)\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1044c0e",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880a9f7a",
   "metadata": {},
   "source": [
    "A sampling function was added to take a random input sequence and use it to sample the trained model's output by converting it back into tokens. This allowed evaluation metrics other than loss and accuracy to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f229ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, dataset):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    if isinstance(dataset, Subset):\n",
    "        dataset = dataset.dataset\n",
    "    idx2char_dict = dataset.idx2char_dict\n",
    "\n",
    "    rand_idx = np.random.randint(len(dataset) - 1)\n",
    "    input_seq, _, input_metadata = dataset[rand_idx]\n",
    "\n",
    "    T = input_metadata.get('T:', \"\")\n",
    "    L = input_metadata.get('L:', \"\")\n",
    "    K = input_metadata.get('K:', \"\")\n",
    "\n",
    "    SPECIAL = {'[PAD]', '[UNK]', '[START]', '[END]'}\n",
    "\n",
    "    metadata_tokens = [f\"T:{T}\", f\"L:{L}\", f\"K:{K}\"]\n",
    "    metadata_indices = [tok2ind(tok) for tok in metadata_tokens]\n",
    "    input_seq = input_seq.unsqueeze(0).to(device)\n",
    "    metadata_tensor = torch.tensor(metadata_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    input_seq = torch.cat([metadata_tensor, input_seq], dim=1)\n",
    "\n",
    "    hidden_state = None\n",
    "\n",
    "    generated = metadata_tokens.copy()\n",
    "    with torch.no_grad():\n",
    "        output, hidden_state = model(input_seq, hidden_state)\n",
    "\n",
    "        last_output = output[:, -1, :]\n",
    "        vals = last_output / HP.temp\n",
    "        probs = F.softmax(vals, dim=1)\n",
    "\n",
    "        dist = Categorical(probs)\n",
    "        next_token = dist.sample()\n",
    "\n",
    "        current_input = next_token.unsqueeze(0)\n",
    "\n",
    "        for _ in range(HP.output_len):\n",
    "            output, hidden_state = model(current_input, hidden_state)\n",
    "            last_output = output[:, -1, :]\n",
    "\n",
    "            vals = last_output / HP.temp\n",
    "            probs = F.softmax(vals, dim=1)\n",
    "\n",
    "            dist = Categorical(probs)\n",
    "            next_token = dist.sample()\n",
    "\n",
    "            token_idx = next_token.item()\n",
    "            if token_idx in idx2char_dict:\n",
    "                tok = idx2char_dict[token_idx]\n",
    "                if tok not in SPECIAL:\n",
    "                    generated.append(tok)\n",
    "\n",
    "            current_input = next_token.unsqueeze(0)\n",
    "\n",
    "    result = ' '.join(generated)\n",
    "\n",
    "    if K and L and T:\n",
    "        key_score = evaluate_key(result, K)\n",
    "        time_sig_score = evaluate_time_signature(result, L, T)\n",
    "        # print(f\"Key: {K}, Score: {key_score:.2%}\")\n",
    "        # print(f\"Time Signature: {T}, Length: {L}, Score: {time_sig_score}\")\n",
    "        return result, key_score, time_sig_score\n",
    "\n",
    "    elif K:\n",
    "        key_score = evaluate_key(result, K)\n",
    "        # print(f\"Key: {K}, Score: {key_score:.2f%}\")\n",
    "        return result, key_score, 0.0\n",
    "\n",
    "    elif L and T:\n",
    "        time_sig_score = evaluate_time_signature(result, L, T)\n",
    "        # print(f\"Time Signature: {T:.2f%}, Length: {L:.2f%}, Score: {time_sig_score:.2f%}\")\n",
    "        return result, 0.0, time_sig_score\n",
    "\n",
    "    return result, 0.0, 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce7892",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc7520",
   "metadata": {},
   "source": [
    "The `build_scale()` function in tests is used for evaluating whether notes in a melody are in-key, but also for transposing the data in the dataset.\n",
    "\n",
    "The `evaluate_key()` functions works by viewing the notes in a melody and checking if they match the notes in the associated key and scale.\n",
    "\n",
    "`evaluate_time_signature()` calculates the duration of the notes in each bar and checks if it is aligned with a song's time signature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcde812",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24ec0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx, dropout):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        # print(\"Input shape:\", x.shape)\n",
    "        # print(\"Input dtype:\", x.dtype)\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        logits = self.fc(output)  # [batch_size, seq_len, vocab_size]\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e7e7453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "CHROMATIC = [\n",
    "    'C', 'C#', 'D', 'D#', 'E', 'F',\n",
    "    'F#', 'G', 'G#', 'A', 'A#', 'B'\n",
    "]\n",
    "\n",
    "NOTES = \"ABCDEFGabcdefg\"\n",
    "\n",
    "TRIPLET_PREFIX = [f\"3{a}{b}{c}\" for a in NOTES for b in NOTES for c in NOTES]\n",
    "\n",
    "EQUIVALENTS = {\n",
    "    'B#': 'C', 'E#': 'F',\n",
    "    'Cb': 'B', 'Fb': 'E',\n",
    "    'Db': 'C#', 'Eb': 'D#', 'Gb': 'F#', 'Ab': 'G#', 'Bb': 'A#',\n",
    "    'C#': 'C#', 'D#': 'D#', 'F#': 'F#', 'G#': 'G#', 'A#': 'A#'\n",
    "}\n",
    "\n",
    "MAJOR_STEPS = [2, 2, 1, 2, 2, 2, 1]\n",
    "MINOR_STEPS = [2, 1, 2, 2, 1, 2, 2]\n",
    "\n",
    "DURATIONS = \"23468\"\n",
    "DOT = \".\"\n",
    "SUFFIXED = [f\"{note}{duration}\" for note in (NOTES) for duration in DURATIONS + DOT]\n",
    "\n",
    "\n",
    "def eq_note(note):\n",
    "    return EQUIVALENTS.get(note, note)\n",
    "\n",
    "\n",
    "def normalize_note(token):\n",
    "\n",
    "    token = token.strip()\n",
    "\n",
    "    match = re.match(r'([_=^]*)([A-Ga-g])', token)\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    accidental, letter = match.groups()\n",
    "    letter = letter.upper()\n",
    "\n",
    "    if accidental == '^':\n",
    "        note = f\"{letter}#\"\n",
    "    elif accidental == '_':\n",
    "        note = f\"{letter}b\"\n",
    "    else:\n",
    "        note = letter\n",
    "\n",
    "    return eq_note(note)\n",
    "\n",
    "\n",
    "def build_scale(key):\n",
    "    is_minor = key.endswith('m')\n",
    "    root = key[:-1] if is_minor else key\n",
    "    root = eq_note(root)\n",
    "\n",
    "    pattern = MINOR_STEPS if is_minor else MAJOR_STEPS\n",
    "\n",
    "    try:\n",
    "        idx = CHROMATIC.index(root)\n",
    "    except ValueError:\n",
    "        # print(f\"Unknown root: {root}\")\n",
    "        return []\n",
    "\n",
    "    scale = [CHROMATIC[idx]]\n",
    "    for step in pattern:\n",
    "        idx = (idx + step) % len(CHROMATIC)\n",
    "        scale.append(CHROMATIC[idx])\n",
    "    return [eq_note(n) for n in scale]\n",
    "\n",
    "\n",
    "def evaluate_key(sample, key):\n",
    "    scale = build_scale(key)\n",
    "    if not scale:\n",
    "        return 0.0\n",
    "\n",
    "    in_key = {}\n",
    "    out_key = {}\n",
    "    total = 0\n",
    "\n",
    "    for raw in sample.split():\n",
    "        note = normalize_note(raw)\n",
    "        if note is None:\n",
    "            continue\n",
    "        total += 1\n",
    "\n",
    "        if note in scale:\n",
    "            in_key[note] = in_key.get(note, 0) + 1\n",
    "        else:\n",
    "            out_key[note] = out_key.get(note, 0) + 1\n",
    "\n",
    "\n",
    "    # output frequencies of in/out key notes\n",
    "\n",
    "    # print(f\"\\nKEY: {key}\\nSCALE: {scale}\")\n",
    "    # if in_key:\n",
    "    #     # print(\"\\nValid note frequencies:\")\n",
    "    #     for note, count in sorted(in_key.items()):\n",
    "    #         print(f\"{note}: {count}\")\n",
    "\n",
    "    # if out_key:\n",
    "    #     # print(\"\\nInvalid note frequencies:\")\n",
    "    #     for note, count in sorted(out_key.items()):\n",
    "    #         print(f\"{note}: {count}\")\n",
    "\n",
    "    score = sum(in_key.values()) / total if total > 0 else 0.0\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluate_time_signature(sample, length, time_sig):\n",
    "    # print(f\"Evaluating time signature: {time_sig}, length: {length}\")\n",
    "    \n",
    "    # Parse base note length (e.g., \"1/8\" -> 0.125)\n",
    "    length = eval(length)  # Convert string fraction to float\n",
    "    \n",
    "    # Convert time signature to total beats per measure\n",
    "    if time_sig == \"C\":\n",
    "        beats_per_bar = 4/4  # 4/4\n",
    "    elif time_sig == \"C|\":\n",
    "        beats_per_bar = 2/2  # 2/2\n",
    "    else:\n",
    "        beats_per_bar = eval(time_sig)  # e.g., \"3/4\" -> 0.75\n",
    "    \n",
    "    bars = [bar.strip() for bar in sample.split('|') if bar.strip()]\n",
    "    correct = 0\n",
    "    \n",
    "    for i, bar in enumerate(bars):\n",
    "        # print(f\"\\n{i} Bar: {bar}\")\n",
    "        total_duration = 0.0\n",
    "\n",
    "        #initial two are definiting time signature and length\n",
    "        i = 2\n",
    "        \n",
    "        while i < len(bar):\n",
    "            # Handle triplets (e.g., \"3DEF\")\n",
    "            if i + 3 <= len(bar) and bar[i] == '3' and bar[i+1] in NOTES and bar[i+2] in NOTES:\n",
    "                total_duration += 2 * length  # 3 notes in time of 2\n",
    "                i += 3\n",
    "            \n",
    "            # Handle dotted notes (e.g., \".D\")\n",
    "            elif i + 1 < len(bar) and bar[i+1] in NOTES and bar[i] == DOT:\n",
    "                total_duration += 1.5 * length  # Original + half\n",
    "                i += 2\n",
    "            \n",
    "            # Handle regular notes with duration (e.g., \"C2\")\n",
    "            elif i + 1 < len(bar) and bar[i] in NOTES and bar[i+1] in DURATIONS:\n",
    "                dur = int(bar[i+1])\n",
    "                total_duration += length * (dur)  # e.g., \"C2\" in 1/8 base = 4/2=2 units\n",
    "                i += 2\n",
    "            \n",
    "            # Handle simple notes (e.g., \"C\")\n",
    "            elif bar[i] in NOTES:\n",
    "                total_duration += length\n",
    "                i += 1\n",
    "            \n",
    "            # Skip rests/other characters\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        # Compare with tolerance for floating-point precision\n",
    "        if abs(total_duration - beats_per_bar) < 0.001:\n",
    "            correct += 1\n",
    "            # print(f\"✓ Correct duration: {total_duration}\")\n",
    "        # else:\n",
    "        #     print(f\"✗ Incorrect duration: {total_duration} (expected {beats_per_bar})\")\n",
    "    \n",
    "    return correct / len(bars) if bars else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74623dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ABCDataset(os.path.join(data_dir, 'lookup_tables', 'songs_dict.json'), augment_data=True)\n",
    "PAD_IDX = dataset.get_pad_idx()\n",
    "train_dataset, val_dataset, test_dataset = dataset.split_dataset(train_ratio=0.8, val_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e05cb",
   "metadata": {},
   "source": [
    "## RNN Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e75d598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T:9/8 L:1/8 K:A g B c3 - A | G2 E E D D | B, F G c A A | d B G A G E | B G A G A G | G A B A F G | D D D D E F | D E D G2 |: | d e f a2 f | g f e g e g | B f B A F A F | G2 G G A G | A B A A B c | a e d c d G G | e A A G F d c d B | A F D F A B c | B g B g2 g | g c d e d c | d2 c f g f | g f. e d f f | a b g e f d | B G B d d B | c A F E2 A | B c d A B2 G F G A | c d B g2 d B | c B c2 c d || e2 | e f d B | c d e f | B2 e g | a g f f e e d\n",
      "Key Score: 57.23%\n",
      "Time Signature Score: 3.12%\n"
     ]
    }
   ],
   "source": [
    "HP.n_layers = 4\n",
    "rnn = RNNModel(VOCAB_SIZE, HP.embed_dim, HP.hidden_dim, HP.n_layers, PAD_IDX, HP.dropout)\n",
    "rnn.load_state_dict(torch.load(os.path.join(output_dir, 'rnn_model.pth'), map_location=torch.device('cpu')))\n",
    "result, key_score, time_sig_score = sample(rnn, test_dataset)\n",
    "print(result)\n",
    "print(f\"Key Score: {key_score:.2%}\")\n",
    "print(f\"Time Signature Score: {time_sig_score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df8340",
   "metadata": {},
   "source": [
    "## GRU Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bbd8b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T:6/8 L:1/8 K:Em f e d B B d | g b b a g f | g f g f g a | f a f g2 a | b a f d d f | 3ABA B G E | E2 E G2 :| | c B E E F G | B A B c d e | f g f e f g f | e d c B2 A | B g e d B A | B B A B2 A | B d g e2 d | g e B A2 :| | c A F E c A | B d e g f e | d B d g2 f | g f e d e c | e2 d f d e | f d c B c d | 3efg f d e B A | B d d A2 b f | e f e d B d | c A F G2 :| | d g a 3gag f a D. b2 | b2 f g f g | a b a g b2 | g e d f e d | ~A F E D2 :| | B A2 d | e f g a b g | d\n",
      "Key Score: 82.94%\n",
      "Time Signature Score: 6.06%\n"
     ]
    }
   ],
   "source": [
    "HP.n_layers = 4\n",
    "gru = GRUModel(VOCAB_SIZE, HP.embed_dim, HP.hidden_dim, HP.n_layers, PAD_IDX, HP.dropout)\n",
    "gru.load_state_dict(torch.load(os.path.join(output_dir, 'gru_model.pth'), map_location=torch.device('cpu')))\n",
    "result, key_score, time_sig_score = sample(gru, test_dataset)\n",
    "print(result)\n",
    "print(f\"Key Score: {key_score:.2%}\")\n",
    "print(f\"Time Signature Score: {time_sig_score:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fedc79",
   "metadata": {},
   "source": [
    "## LSTM Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36f6e0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T:6/8 L:1/8 K:G | G B c d B G | G F G E F G | A3 B d c | d e d c A B | c B A F D F E | D E A d c d | e d c f2 g | a g f e2 a g | d c A B d c | A G E G2 :| A | B G E G B d | f e f g2 f | e d c d2 B d | c B A G F E | G E D G2 A | A G G A2 G | B G G G2 :| d | g2 g g a g | f d f e c A | d2 B A B c | d g g g g a | b e g f g f | | e f g f2 f | f d c B G g | g e c d2 :| | G c d e f g | f g a b g e | f d B g e c | d d d 3gag f | e d A D A F :| :| g a g g e f | g f g f e d | c\n",
      "Key Score: 85.47%\n",
      "Time Signature Score: 10.81%\n"
     ]
    }
   ],
   "source": [
    "HP.n_layers = 2\n",
    "\n",
    "lstm = LSTMModel(VOCAB_SIZE, HP.embed_dim, HP.hidden_dim, HP.n_layers, PAD_IDX, HP.dropout)\n",
    "lstm.load_state_dict(torch.load(os.path.join(output_dir, 'lstm_model.pth'), map_location=torch.device('cpu')))\n",
    "result, key_score, time_sig_score = sample(lstm, test_dataset)\n",
    "print(result)\n",
    "print(f\"Key Score: {key_score:.2%}\")\n",
    "print(f\"Time Signature Score: {time_sig_score:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
